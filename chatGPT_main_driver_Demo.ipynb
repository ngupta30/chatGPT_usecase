{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8165a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import GPT2TokenizerFast\n",
    "from typing import List, Dict, Tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76bd09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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'\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "\n",
    "# models for embedding\n",
    "MODEL_NAME = \"curie\"\n",
    "\n",
    "DOC_EMBEDDINGS_MODEL = f\"text-search-{MODEL_NAME}-doc-001\"\n",
    "QUERY_EMBEDDINGS_MODEL = f\"text-search-{MODEL_NAME}-query-001\"\n",
    "\n",
    "header = ''\n",
    "#\"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd9b3a",
   "metadata": {},
   "source": [
    "### Using the large context file using qry_passage_match = False: \n",
    "qry_passage_match sets which method to choose:\n",
    "qry_passage_match = False means that whole context will be divided in multiple paragraphs with overlapping tokens based on max_promt_len acceptable by the model. answer is extracted from each paragraph and then further these answers are combined and used as prompt to chatGPT and get final answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e3522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing --- start 2023-02-16 14:47:39.381750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.txt\n",
      "when this agreement will be terminated?\n",
      " This Agreement will be terminated upon the death of the Employee, the termination of the Employee's employment, or the cancellation of the Policies.\n",
      " This agreement will be terminated when the employee is no longer employed by the employer, or when the employee or employer decides to terminate the agreement.\n",
      " This Agreement may be terminated at any time while the Employee is living by written notice thereof by either the Employer or the Employee to the other, and, in any event, this Agreement will terminate upon termination of the Employee's employment.\n",
      "\n",
      " Q: when this agreement will be terminated?\n",
      " A:\n",
      "99\n",
      "{'temperature': 0.0, 'max_tokens': 300, 'model': 'text-davinci-003'}\n",
      "This agreement is governed by which law?\n",
      " This agreement is governed by the laws of the state in which the Employer is located.\n",
      " This Agreement will be governed by the laws of the State of Pennsylvania.\n",
      "\n",
      " Q: This agreement is governed by which law?\n",
      " A:\n",
      "39\n",
      "{'temperature': 0.0, 'max_tokens': 300, 'model': 'text-davinci-003'}\n",
      "[\"This Agreement may be terminated at any time while the Employee is living by written notice thereof by either the Employer or the Employee to the other, and, in any event, this Agreement will terminate upon termination of the Employee's employment.\", 'This Agreement will be governed by the laws of the State of Pennsylvania.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import main_driver as chatGPT\n",
    "\n",
    "event = {'context_filepath': 'sample.txt',\n",
    "             'ques_filepath': 'ques.json',\n",
    "             'output_dir': 'output',\n",
    "             'doc_embed_model': DOC_EMBEDDINGS_MODEL,\n",
    "             'qry_embed_model': QUERY_EMBEDDINGS_MODEL,\n",
    "             'completion_model': COMPLETIONS_MODEL,\n",
    "             'max_prompt_len':4000,\n",
    "             'temperature': 0.0, \n",
    "             'max_tokens': 300,\n",
    "             'qry_passage_match': False,\n",
    "             'doc_stride':100,\n",
    "             'header':header\n",
    "        }\n",
    "chatGPT.event_handler(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0179a",
   "metadata": {},
   "source": [
    "Ans to both questions were answered correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f70ed",
   "metadata": {},
   "source": [
    "### Using the small context file using qry_passage_match = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ddd3963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing --- start 2023-02-16 14:13:25.857101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_short.txt\n",
      "when this agreement will be terminated?\n",
      "Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\n",
      "\n",
      "Context:\n",
      " This Agreement may be terminated at any time while the Employee is living by written notice thereof by either the Employer or the Employee to the other, and, in any event, this Agreement will terminate upon termination of the Employee's employment.\n",
      "\n",
      " Q: when this agreement will be terminated?\n",
      " A:\n",
      "75\n",
      "{'temperature': 0.0, 'max_tokens': 300, 'model': 'text-davinci-003'}\n",
      "This agreement is governed by which law?\n",
      "Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\n",
      "\n",
      "Context:\n",
      " This Agreement will be governed by the laws of the State of Pennsylvania.\n",
      "\n",
      " Q: This agreement is governed by which law?\n",
      " A:\n",
      "48\n",
      "{'temperature': 0.0, 'max_tokens': 300, 'model': 'text-davinci-003'}\n",
      "[\"Upon termination of the Employee's employment.\", 'The laws of the State of Pennsylvania.']\n"
     ]
    }
   ],
   "source": [
    "event = {'context_filepath': 'sample_short.txt',\n",
    "             'ques_filepath': 'ques.json',\n",
    "             'output_dir': 'output',\n",
    "             'doc_embed_model': DOC_EMBEDDINGS_MODEL,\n",
    "             'qry_embed_model': QUERY_EMBEDDINGS_MODEL,\n",
    "             'completion_model': COMPLETIONS_MODEL,\n",
    "             'max_prompt_len':4000,\n",
    "             'temperature': 0.0, \n",
    "             'max_tokens': 300,\n",
    "             'qry_passage_match': False,\n",
    "             'doc_stride':100,\n",
    "             'header':header\n",
    "        }\n",
    "chatGPT.event_handler(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9fe063",
   "metadata": {},
   "source": [
    "Ans to both questions were answered correctly and also more clear ans was generated only one time directly from passage.\n",
    "\n",
    "### Using the small context file using qry_passage_match = True¶:\n",
    "qry_passage_match = True means that context will be divided into paragraphs based on new line and then paragraphs embdding will be calculated against query emb. Top paragraphs will be accumulated till the max_prompt_len token limit has been hit and this context is used as prompt for chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3de5463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing --- start 2023-02-16 14:53:27.708885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_short2.txt\n",
      "when this agreement will be terminated?\n",
      "Selected 22 document sections:\n",
      "7\n",
      "6\n",
      "5\n",
      "15\n",
      "14\n",
      "1\n",
      "18\n",
      "10\n",
      "19\n",
      "11\n",
      "0\n",
      "2\n",
      "9\n",
      "8\n",
      "12\n",
      "16\n",
      "4\n",
      "13\n",
      "21\n",
      "17\n",
      "20\n",
      "3\n",
      "\n",
      "*  otherÍ¾ and, in any event, this Agreement will terminate upon termination of the Employee's employment.\n",
      "*  This Agreement may be terminated at any time while the Employee is living by written notice thereof by either the Employer or the Employee to the\n",
      "*  \"Termination of Agreement\"\n",
      "*  hereby superseded. This Agreement will be governed by the laws of the State of Pennsylvania.\n",
      "*  This Agreement sets forth the entire Agreement of the parties hereto, and any and all prior agreements, to the extent inconsistent herewith, are\n",
      "*  THIS AGREEMENT is made and entered into this 1st day of June, 2017 by and between Prudential Bank (hereinafter referred to as the\n",
      "*  Where appropriate in this Agreement, words used in the singular will include the plural and words used in the masculine will include the feminine.\n",
      "*  The Employer will keep possession of the Policies. The Employer agrees from time to time to make the policies available to the Employee or to the\n",
      "*  IN WITNESS WHEREOF, the parties have hereunto set their hand, the Employer by its duly authorized officer, on the day and year first above\n",
      "*  \"Insurer\" for the purpose of endorsing or filing any change of beneficiary on the Policies but the Policies will promptly be returned to the Employer.\n",
      "* Split-Dollar Endorsement Agreement\n",
      "*  \"Employer\"), located in Philadelphia, Pennsylvania and Jeffrey Hanuscin, (hereinafter referred to as the \"Employee\"), residing at 2406 Sanibel Circle,\n",
      "*  \"Possession of Policies\"\n",
      "*  ARTICLE XI\n",
      "*  ARTICLE XII\n",
      "*  ARTICLE XIII\n",
      "*  ARTICLE VI\n",
      "*  \"Governing Law\"\n",
      "* \n",
      "*  \"Interpretation\"\n",
      "*  written.\n",
      "*  \n",
      "\n",
      " Q: when this agreement will be terminated?\n",
      " A:\n",
      "272\n",
      "{'temperature': 0.0, 'max_tokens': 300, 'model': 'text-davinci-003'}\n",
      "This agreement is governed by which law?\n",
      "Selected 22 document sections:\n",
      "15\n",
      "14\n",
      "18\n",
      "7\n",
      "6\n",
      "1\n",
      "13\n",
      "10\n",
      "19\n",
      "2\n",
      "11\n",
      "0\n",
      "5\n",
      "9\n",
      "8\n",
      "16\n",
      "12\n",
      "4\n",
      "17\n",
      "21\n",
      "20\n",
      "3\n",
      "\n",
      "*  hereby superseded. This Agreement will be governed by the laws of the State of Pennsylvania.\n",
      "*  This Agreement sets forth the entire Agreement of the parties hereto, and any and all prior agreements, to the extent inconsistent herewith, are\n",
      "*  Where appropriate in this Agreement, words used in the singular will include the plural and words used in the masculine will include the feminine.\n",
      "*  otherÍ¾ and, in any event, this Agreement will terminate upon termination of the Employee's employment.\n",
      "*  This Agreement may be terminated at any time while the Employee is living by written notice thereof by either the Employer or the Employee to the\n",
      "*  THIS AGREEMENT is made and entered into this 1st day of June, 2017 by and between Prudential Bank (hereinafter referred to as the\n",
      "*  \"Governing Law\"\n",
      "*  The Employer will keep possession of the Policies. The Employer agrees from time to time to make the policies available to the Employee or to the\n",
      "*  IN WITNESS WHEREOF, the parties have hereunto set their hand, the Employer by its duly authorized officer, on the day and year first above\n",
      "*  \"Employer\"), located in Philadelphia, Pennsylvania and Jeffrey Hanuscin, (hereinafter referred to as the \"Employee\"), residing at 2406 Sanibel Circle,\n",
      "*  \"Insurer\" for the purpose of endorsing or filing any change of beneficiary on the Policies but the Policies will promptly be returned to the Employer.\n",
      "* Split-Dollar Endorsement Agreement\n",
      "*  \"Termination of Agreement\"\n",
      "*  \"Possession of Policies\"\n",
      "*  ARTICLE XI\n",
      "*  ARTICLE XIII\n",
      "*  ARTICLE XII\n",
      "*  ARTICLE VI\n",
      "*  \"Interpretation\"\n",
      "* \n",
      "*  written.\n",
      "*  \n",
      "\n",
      " Q: This agreement is governed by which law?\n",
      " A:\n",
      "273\n",
      "{'temperature': 0.0, 'max_tokens': 300, 'model': 'text-davinci-003'}\n",
      "['This Agreement may be terminated at any time while the Employee is living by written notice thereof by either the Employer or the Employee.', 'This Agreement will be governed by the laws of the State of Pennsylvania.']\n"
     ]
    }
   ],
   "source": [
    "event = {'context_filepath': 'sample_short2.txt',\n",
    "             'ques_filepath': 'ques.json',\n",
    "             'output_dir': 'output',\n",
    "             'doc_embed_model': DOC_EMBEDDINGS_MODEL,\n",
    "             'qry_embed_model': QUERY_EMBEDDINGS_MODEL,\n",
    "             'completion_model': COMPLETIONS_MODEL,\n",
    "             'max_prompt_len':4000,\n",
    "             'temperature': 0.0, \n",
    "             'max_tokens': 300,\n",
    "             'qry_passage_match': True,\n",
    "             'doc_stride':100,\n",
    "              'header': header\n",
    "        }\n",
    "chatGPT.event_handler(event)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28d3b962",
   "metadata": {},
   "source": [
    "Ans to both questions were answered correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba485a6",
   "metadata": {},
   "source": [
    "large context file is not working for embedding model in free version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a68cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing --- start 2023-02-16 14:18:02.231264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.txt\n",
      "when this agreement will be terminated?\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for default-global-with-image-limits in organization org-G81laINrz8uYRxty9ToODqzM on requests per min. Limit: 60.000000 / min. Current: 70.000000 / min. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21844/651072724.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m              \u001b[1;34m'doc_stride'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         }\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mchatGPT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive - KPMG\\Documents\\Projects\\NLP\\chatGPT_api\\main_driver.py\u001b[0m in \u001b[0;36mevent_handler\u001b[1;34m(event)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mques_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ques'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchatGPT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_ans_long_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mans_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - KPMG\\Documents\\Projects\\NLP\\chatGPT_api\\chatGPT_embedding_class.py\u001b[0m in \u001b[0;36mget_ans_long_context\u001b[1;34m(self, context, ques)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mpara_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpara_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m# calculate embeddings for paragraphs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m             \u001b[0mpara_embed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_doc_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpara_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;31m# calculate similarity against query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - KPMG\\Documents\\Projects\\NLP\\chatGPT_api\\chatGPT_embedding_class.py\u001b[0m in \u001b[0;36mcompute_doc_embeddings\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         return {\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDOC_EMBEDDINGS_MODEL\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         }\n",
      "\u001b[1;32m~\\OneDrive - KPMG\\Documents\\Projects\\NLP\\chatGPT_api\\chatGPT_embedding_class.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         return {\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDOC_EMBEDDINGS_MODEL\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         }\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - KPMG\\Documents\\Projects\\NLP\\chatGPT_api\\chatGPT_embedding_class.py\u001b[0m in \u001b[0;36mget_embedding\u001b[1;34m(self, text, model)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         result = openai.Embedding.create(\n\u001b[0m\u001b[0;32m     32\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m           input=text)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\openai\\api_resources\\embedding.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[1;31m# If a user specifies base64, we'll just return the encoded string.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    151\u001b[0m         )\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         )\n\u001b[1;32m--> 227\u001b[1;33m         \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m             return (\n\u001b[1;32m--> 620\u001b[1;33m                 self._interpret_response_line(\n\u001b[0m\u001b[0;32m    621\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m             raise self.handle_error_response(\n\u001b[0m\u001b[0;32m    681\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m             )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Rate limit reached for default-global-with-image-limits in organization org-G81laINrz8uYRxty9ToODqzM on requests per min. Limit: 60.000000 / min. Current: 70.000000 / min. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."
     ]
    }
   ],
   "source": [
    "event = {'context_filepath': 'sample.txt',\n",
    "             'ques_filepath': 'ques.json',\n",
    "             'output_dir': 'output',\n",
    "             'doc_embed_model': DOC_EMBEDDINGS_MODEL,\n",
    "             'qry_embed_model': QUERY_EMBEDDINGS_MODEL,\n",
    "             'completion_model': COMPLETIONS_MODEL,\n",
    "             'max_prompt_len':4000,\n",
    "             'temperature': 0.0, \n",
    "             'max_tokens': 300,\n",
    "             'qry_passage_match': True,\n",
    "             'doc_stride':100\n",
    "        }\n",
    "chatGPT.event_handler(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f387c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the class functionality\n",
    "# from chatGPT_embedding_class import chatGPT_QA\n",
    "\n",
    "# context_file = 'sample.txt'\n",
    "# with open(context_file) as f:\n",
    "#     lines = f.readlines()\n",
    "# context = ' '.join(lines)\n",
    "\n",
    "# chat_class = chatGPT_QA(COMPLETIONS_MODEL,MAX_SECTION_LEN =4000, qry_passage_match=False)\n",
    "# ans = chat_class.get_ans_long_context(context, 'what is effect of termination?')#'This agreement is governed by which law?')\n",
    "# ans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
